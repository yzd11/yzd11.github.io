<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="referrer" content="no-referrer"><title>Transformer - yzd&#039;s Log Book</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="yzd&#039;s Log Book"><meta name="msapplication-TileImage" content="/img/redhat.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="yzd&#039;s Log Book"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="了解 Transformer"><meta property="og:type" content="blog"><meta property="og:title" content="Transformer"><meta property="og:url" content="https://yzd.life/2024/11/11/Transformer/"><meta property="og:site_name" content="yzd&#039;s Log Book"><meta property="og:description" content="了解 Transformer"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731059389533-85d83535-8421-4242-8f08-9842d3c998c6.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731059326404-b45ea056-7ed5-473f-b999-b6f47fa189e0.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731059906943-1c59ae29-dbcf-43a8-a18e-888f6838b592.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731060100178-630aa339-7d87-4307-a7cd-97e2d3503e80.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731060919570-1b51aba2-2883-4459-a5f8-9236962b6f6d.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731061011263-f02b248e-e763-4ce4-917f-9473009da230.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731061060487-41b8e494-b400-481f-9a12-479b8d421005.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731061315933-34118ebc-1081-4af4-a8df-ef92b8cf2787.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731061526714-74569b81-d04e-4183-a87a-88871709ef3b.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731072669558-64ca169a-17df-4146-b73e-c77bf9503add.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731072933162-e6701fa0-39a3-4f66-b8e1-ef9054503fed.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731073263917-2c373f2e-f5fa-43d5-81c1-4437ef3d3c70.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731073335145-690059e5-a58a-4df8-b701-16ba714f9182.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731073494959-55e7c24b-fb9d-4003-8166-253112c454f6.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731073768425-f6bc2f7e-ee3a-4c2e-8054-68d813d0a62b.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731073856502-18a46e15-c71a-4a79-a87f-6b49ea3d93f4.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731074351623-4a381f46-b5cf-4310-9f39-b59bc205158f.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731074750520-fa17f636-eb9f-475f-9f18-0051348173f6.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731074923060-1c0b588d-c98b-4f65-aa8a-6bf45e491628.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731075200165-d0165207-02e9-4a7a-81c6-04743ff4720e.png"><meta property="og:image" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731075824324-2a1a2748-a436-4389-a64f-8a988124ee91.png"><meta property="article:published_time" content="2024-11-11T03:57:52.000Z"><meta property="article:modified_time" content="2024-11-11T05:54:53.169Z"><meta property="article:author" content="yzd"><meta property="article:tag" content="Deep Learning"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731059389533-85d83535-8421-4242-8f08-9842d3c998c6.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://yzd.life/2024/11/11/Transformer/"},"headline":"Transformer","image":["https://cdn.nlark.com/yuque/0/2024/png/40370285/1731059389533-85d83535-8421-4242-8f08-9842d3c998c6.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731059326404-b45ea056-7ed5-473f-b999-b6f47fa189e0.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731059906943-1c59ae29-dbcf-43a8-a18e-888f6838b592.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731060100178-630aa339-7d87-4307-a7cd-97e2d3503e80.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731060919570-1b51aba2-2883-4459-a5f8-9236962b6f6d.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731061011263-f02b248e-e763-4ce4-917f-9473009da230.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731061060487-41b8e494-b400-481f-9a12-479b8d421005.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731061315933-34118ebc-1081-4af4-a8df-ef92b8cf2787.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731061526714-74569b81-d04e-4183-a87a-88871709ef3b.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731072669558-64ca169a-17df-4146-b73e-c77bf9503add.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731072933162-e6701fa0-39a3-4f66-b8e1-ef9054503fed.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731073263917-2c373f2e-f5fa-43d5-81c1-4437ef3d3c70.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731073335145-690059e5-a58a-4df8-b701-16ba714f9182.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731073494959-55e7c24b-fb9d-4003-8166-253112c454f6.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731073768425-f6bc2f7e-ee3a-4c2e-8054-68d813d0a62b.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731073856502-18a46e15-c71a-4a79-a87f-6b49ea3d93f4.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731074351623-4a381f46-b5cf-4310-9f39-b59bc205158f.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731074750520-fa17f636-eb9f-475f-9f18-0051348173f6.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731074923060-1c0b588d-c98b-4f65-aa8a-6bf45e491628.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731075200165-d0165207-02e9-4a7a-81c6-04743ff4720e.png","https://cdn.nlark.com/yuque/0/2024/png/40370285/1731075824324-2a1a2748-a436-4389-a64f-8a988124ee91.png"],"datePublished":"2024-11-11T03:57:52.000Z","dateModified":"2024-11-11T05:54:53.169Z","author":{"@type":"Person","name":"yzd"},"publisher":{"@type":"Organization","name":"yzd's Log Book","logo":{"@type":"ImageObject","url":"https://yzd.life/img/redhat.svg"}},"description":"了解 Transformer"}</script><link rel="canonical" href="https://yzd.life/2024/11/11/Transformer/"><link rel="icon" href="/img/redhat.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/redhat.svg" alt="yzd&#039;s Log Book" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/yzd11/"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-11-11T03:57:52.000Z" title="2024/11/11 11:57:52">2024-11-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-11-11T05:54:53.169Z" title="2024/11/11 13:54:53">2024-11-11</time></span><span class="level-item"><a class="link-muted" href="/categories/blog/">blog</a></span><span class="level-item">19 minutes read (About 2844 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Transformer</h1><div class="content"><p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731059389533-85d83535-8421-4242-8f08-9842d3c998c6.png"></p>
<p>对于$Transformer$来讲，其中主要分为$Encoder$和$Decoder$，接下来首先介绍$Encoder$。</p>
<h2 id="GnX9y">Encoder</h2>

<p>$Encoder$的主要作用是进行特征提取，这样做是因为原始输入中包含一些无用或干扰信息，这会使模型的性能和泛化性大打折扣。所以在这之前，我们通过$ Encoder $来先对数据进行一次特征提取和挖掘，比如后面会提到$ Encoder $里会有一个自注意力层，正如我们之前文章中提到，自注意力层可以挖掘输入内部元素直接的关系，这是模型直接接受原始输入很难做到。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731059326404-b45ea056-7ed5-473f-b999-b6f47fa189e0.png"></p>
<p>$ Encoder $可以给定一排向量，输出一排同样的向量，对于这个要求$ RNN,CNN $都可以实现，而在$ Transformer $中主要使用的是$ self-attention $，但是这个模型有些复杂，接下来分步介绍这个$ Encoder $。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731059906943-1c59ae29-dbcf-43a8-a18e-888f6838b592.png"></p>
<p>当向量输入之后要经过多个$ Block $，但是每个$ Block $并不是一层网络，而是一个$ self-attention $加上一个$ FC $。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731060100178-630aa339-7d87-4307-a7cd-97e2d3503e80.png"></p>
<p>但是这里的$ self-attention $跟之前的有些不同，还需要加上$ residual $和$ norm $，同时在$ FC $也加上这两个操作。需要注意的是这里的是层归一化。</p>
<p><strong>层归一化和批量归一化：</strong></p>
<p>$ BatchNorm $<font style="color:rgb(77, 77, 77);">把一个</font>$ batch $<font style="color:rgb(77, 77, 77);">中同一通道的所有特征（如下图红色区域）视为一个分布（有几个通道就有几个分布），并将其标准化。这意味着:</font></p>
<ul>
<li><font style="color:rgba(0, 0, 0, 0.75);">不同图片的的同一通道的相对关系是保留的，即不同图片的同一通道的特征是可以比较的</font></li>
<li><font style="color:rgba(0, 0, 0, 0.75);">同一图片的不同通道的特征则是失去了可比性</font></li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731060919570-1b51aba2-2883-4459-a5f8-9236962b6f6d.png"></p>
<p>$ LayerNorm $把一个样本的所有词义向量（如下图红色部分）视为一个分布（有几个句子就有几个分布），并将其标准化。这意味着:</p>
<p>同一句子中词义向量（上图中的$ V1, V2, …, VL $）的相对大小是保留的，或者也可以说$ LayerNorm $不改变词义向量的方向，只改变它的模。不同句子的词义向量则是失去了可比性。</p>
<p>用于$ NLP $领域解释:</p>
<p>考虑两个句子，“教练，我想打篮球！” 和 “老板，我要一打包子。”。通过比较两个句子中 “打” 的词义我们可以发现，词义并非客观存在的，而是由上下文的语义决定的。因此进行标准化时不应该破坏同一句子中不同词义向量的可比性，而$ LayerNorm $是满足这一点的，$ BatchNorm $则是不满足这一点的。且不同句子的词义特征也不应具有可比性，$ LayerNorm $也是能够把不同句子间的可比性消除。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731061011263-f02b248e-e763-4ce4-917f-9473009da230.png"></p>
<p>$ Transformer $使用$ LayerNorm $而不是$ BatchNorm $的主要原因是$ LayerNorm $更加适合处理变长的序列数据。在$ Transformer $中，由于输入序列的长度是可变的，因此每个序列的批量统计信息（如均值和方差）也是不同的。而$ BatchNorm $是基于整个批量数据来计算统计信息的，这可能导致在处理变长序列时性能下降。相比之下，$ LayerNorm $是在每个样本的维度上进行归一化的，因此不受序列长度变化的影响。</p>
<p>在$ Transformer $中，$ LayerNorm $通常被放置在多头注意力机制和前馈神经网络的输出之后。通过对这些层的输出进行归一化，可以使得后续层的输入保持在一个相对稳定的范围内，有助于模型的训练。</p>
<p><font style="color:rgba(0, 0, 0, 0.75);">简单对比如下：</font></p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731061060487-41b8e494-b400-481f-9a12-479b8d421005.png"></p>
<p>参照于：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_48086806/article/details/132153059">【深度学习中的批量归一化BN和层归一化LN】BN层（Batch Normalization）和LN层（Layer Normalization）的区别_bn和ln-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44174227/article/details/141558713">Transformer 系列三：Encoder编码器和Decoder解码器_transformer编码器-CSDN博客</a></p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731061315933-34118ebc-1081-4af4-a8df-ef92b8cf2787.png"></p>
<p>对比一下刚刚看到的模型，只是将$ Self-Attention $换成了$ Mult-Head\  Attention $，同时在输入之前加上了位置编码。</p>
<p>同时将全连接层换成了一个前馈神经网络，其实就是一个简单的两层全连接网络，用于进一步处理多头注意力机制的输出。这个网络首先通过一个线性变换将输入映射到一个更高维的空间中，然后通过一个非线性激活函数（如$ ReLU $）增加网络的非线性能力，最后再通过另一个线性变换将输出映射回原始维度。</p>
<p>在自注意力层后面增加前馈神经网络层的原因：</p>
<ol>
<li>特征提取：$ FFN $通过两层全连接层（通常是线性层），对来自自注意力层的输出进行进一步的特征提取。这有助于模型学习到更深层次的、非线性的特征表示。</li>
<li>增加模型容量：通过引入额外的参数和非线性激活函数，$ FFN $增加了模型的容量，使得模型能够捕捉更复杂的数据模式和关系。</li>
<li>与自注意力机制互补：自注意力机制擅长捕捉序列内部的长距离依赖关系，而$ FFN $则专注于在给定的表示上进行特征转换。这种组合使得$ Transformer $能够有效地处理各种语言和序列任务。</li>
<li>提高泛化能力：$ FFN $通过增加模型的复杂性，有助于提高模型对未见数据的泛化能力。</li>
</ol>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731061526714-74569b81-d04e-4183-a87a-88871709ef3b.png"></p>
<p>其实$ Encoder $的架构并不是一定要这样设计，我们可以将其中的归一化的操作提前进行，这样的效果似乎看起来更好。</p>
<h2 id="LIDJk">Decoder</h2>

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731072669558-64ca169a-17df-4146-b73e-c77bf9503add.png"></p>
<p>我们刚才介绍了$ Encoder $，现在我们先不考虑$ Encoder $的输出怎么作为$ Decoder $的输入，现在假设已经可以传递。首先使用一个$ BEGIN $作为一个输入，同时根据这个输入得到一个输出，经过一个$ softmax $之后得到一个表，选择其中概率最大的值，将这个作为我们的预测结果，同时这个表的长度就是我们希望的词的范围大小，可能是中文的常用词，大概$ 3000,5000 $字左右，也可能是例如英文的常用词的大小。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731072933162-e6701fa0-39a3-4f66-b8e1-ef9054503fed.png">接着使用上次的输出作为这次的输入，对于如果中间出现了错误是否会导致接下来的结果全部错误再下面会提到。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731073263917-2c373f2e-f5fa-43d5-81c1-4437ef3d3c70.png"></p>
<p>这个是$ Decoder $的结构图，看起来比$ Encoder $还要复杂，但是如果把中间的部分删除，其实别的部分基本一样。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731073335145-690059e5-a58a-4df8-b701-16ba714f9182.png"></p>
<p>可以看到，其余部分基本上一样，并没有什么很大的差别，只是其中的$ Multi-Head\ Attention $换成了$ Masked \  Multi-Head \ Attention $。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731073494959-55e7c24b-fb9d-4003-8166-253112c454f6.png"></p>
<p>对于$ Masked \  Multi-Head \ Attention $来讲，其中的主要区别就是，当$ q^2 $是，对应的$ k $只有$ k^1,k^2 $，只可以查询到当前和再前面的值，其实从刚才的$ Decoder $的原理也可以理解，其中的$ a^1,a^2,a^3,a^4 $并不是同时出现的，因此需要使用$ Masked $的形式。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731073768425-f6bc2f7e-ee3a-4c2e-8054-68d813d0a62b.png">刚才的$ Decoder $只有开始没有结束，为了可以让机器自己学习怎么结束，我们可以通过使用加上一个$ END $在表中的方式，其中当输出为$ END $的时候表示结束。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731073856502-18a46e15-c71a-4a79-a87f-6b49ea3d93f4.png"></p>
<p>如图中所示，当输入“习”这个词的时候会输出一个$ END $表示结束。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731074351623-4a381f46-b5cf-4310-9f39-b59bc205158f.png"></p>
<p>现在对刚才讲的$ Decoder $做一个总结，刚才讲的$ Decoder $全是$ AT\  Eecoder $的形式，但是还有一种$ NAT $的形式，也就是不再一个一个输入，而是采用一次性同时输入的形式，但是这样需要采用一定的措施，例如使用一个新的预测将会有几个输出，或者直接输入很多的$ BEGIN $，然后根据$ END $截断，只要$ END $前面的。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731074750520-fa17f636-eb9f-475f-9f18-0051348173f6.png"></p>
<p>现在需要介绍$ Decoder $是怎么使用$ Encoder $的信息的，通过经过$ Masked \  Multi-Head \ Attention $生成的向量得到一个$ q $，然后使用这个$ q $和$ Encoder $计算得到一个$ v $作为接下来全连接层的输入。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731074923060-1c0b588d-c98b-4f65-aa8a-6bf45e491628.png"></p>
<p>而且未必一定要用$ Encoder $的最后一层的输出作为输入，还可以使用别的方式进行信息的传递。</p>
<h2 id="IDuWG">Training</h2>

<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731075200165-d0165207-02e9-4a7a-81c6-04743ff4720e.png"></p>
<p>刚才介绍的都是我们已经训练好了一个模型，然后测试这个模型。但是现在要介绍的是怎么训练一个模型，首先我们需要人为的加上标签，也就是使用$ one-hot $表示，同时最小化输出和标签之间的$ cross \ entropy $，同时需要注意的是刚才提的到那个问题，如果在$ Decoder $中预测数据不正确怎么办，因此我们需要使用$ Ground\  Truth $，也就是正确的数据作为输入。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2024/png/40370285/1731075824324-2a1a2748-a436-4389-a64f-8a988124ee91.png"></p>
<p>刚才的方法还存在一个曝光偏差($ exposure \ bias  $)，训练($ training $) 时接受的标签是真实的值($ ground\  truth\  input $)，但测试 ($ testing $) 时却接受自己前一个单元的输出($ output $)作为本单元的输入($ input $)，这两个$ setting $不一致会导致误差累积。</p>
<p>解决这个问题也不麻烦，只要在训练的时候在正确的标签中故意加入一些错误的数据即可。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Transformer</p><p><a href="https://yzd.life/2024/11/11/Transformer/">https://yzd.life/2024/11/11/Transformer/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>yzd</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2024-11-11</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2024-11-11</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Deep-Learning/">Deep Learning</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2024/10/21/Self-Attention/"><span class="level-item">Self-Attention</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.jpg" alt="yzd"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">yzd</p><p class="is-size-6 is-block">Artificial Intelligence Machine Learning Computer Science</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>郑州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Category</p><a href="/categories"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tag</p><a href="/tags"><p class="title">1</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/yzd11/" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/yzd11/"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="语雀" href="https://www.yuque.com/yzd11/myblog/"><i class="fa-solid fa-feather"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="CSDN" href="https://blog.csdn.net/yzd111/"><i class="fa-solid fa-c"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Blog" href="https://yzd.life/"><i class="fa-brands fa-blogger-b"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#GnX9y"><span class="level-left"><span class="level-item">1</span><span class="level-item">Encoder</span></span></a></li><li><a class="level is-mobile" href="#LIDJk"><span class="level-left"><span class="level-item">2</span><span class="level-item">Decoder</span></span></a></li><li><a class="level is-mobile" href="#IDuWG"><span class="level-left"><span class="level-item">3</span><span class="level-item">Training</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/blog/"><span class="level-start"><span class="level-item">blog</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-11-11T03:57:52.000Z">2024-11-11</time></p><p class="title"><a href="/2024/11/11/Transformer/">Transformer</a></p><p class="categories"><a href="/categories/blog/">blog</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-21T02:57:51.000Z">2024-10-21</time></p><p class="title"><a href="/2024/10/21/Self-Attention/">Self-Attention</a></p><p class="categories"><a href="/categories/blog/">blog</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-11T04:57:51.000Z">2024-10-11</time></p><p class="title"><a href="/2024/10/11/RNN/">RNN</a></p><p class="categories"><a href="/categories/blog/">blog</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-21T04:51:47.000Z">2024-09-21</time></p><p class="title"><a href="/2024/09/21/%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">训练神经网络</a></p><p class="categories"><a href="/categories/blog/">blog</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-17T01:57:51.000Z">2024-09-17</time></p><p class="title"><a href="/2024/09/17/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%EF%BC%9AKNN%E5%92%8C%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/">图像分类:KNN和线性分类器</a></p><p class="categories"><a href="/categories/blog/">blog</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/11/"><span class="level-start"><span class="level-item">November 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/10/"><span class="level-start"><span class="level-item">October 2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">September 2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/07/"><span class="level-start"><span class="level-item">July 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">6</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/redhat.svg" alt="yzd&#039;s Log Book" height="28"></a><p class="is-size-7"><span>&copy; 2024 yzd</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">用💖发电</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/yzd11/"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>