{"posts":[{"title":"RNN","text":"介绍 ![](https://cdn.nlark.com/yuque/0/2024/png/40370285/1729755749283-e3639f9b-1336-4ff8-8d55-ae10cf4d0ca1.png) 对用同一个词语在不同的话中可能具有完全相反的意思，但是如果还是使用之前神经网络，那么输入结果一定是相同的，为了解决这个问题，我们希望神经网络可以具有一些记忆性，这就是接下来要介绍的循环神经网络。 因此我们讲隐藏层的输出结果全部都放入到一个$ store $中进行存储，在下次输入中再取出来进行使用。 用一个例子解释，当输入为$ [1,1] $，权重为$ 1 $，无偏置的情况，第一次的输出就是$ [4,4] $，同时隐藏层的$ [2,2] $存储在$ store $中。 在第二次中，使用了上一次的存储值，因此就是相当于有四个输入$ 1,1,2,2 $因此最终即使输入相同，但是输出也不同。 需要注意的是，输入的顺序不同，那么得到的结果也会有所不同。 刚才讲的是$ Elman\\ Network $，即存储隐藏层的数据，还可以像$ jordan\\ Network $一样存储上一次的输出层的结果。 除了刚才的神经网络，还有别的变形，例如上图的双向循环神经网络。 LSTM ![](https://cdn.nlark.com/yuque/0/2024/png/40370285/1729756559765-ef7ed7b0-2443-4274-a2b9-29cd1db4d2fc.png) 除了像刚刚讲到的最基本的$ RNN $，还可以对它的存储内容进行调整。使用$ Input\\ Gate $作为输入控制，判断是否可以讲结果存储$ store $中，使用$ Forget\\ Gate $作为判断是否要将存储的数据清空，使用$ Output\\ Gate $作为输出控制，判断别的网络是否可以使用这个存储的结果。 有一个冷知识，这个名字长短时记忆网络的断句是长 短时记忆网络，是较长的短时记忆网络，因为比最原始的$ RNN $保留的时间长一些。 刚才提到的思路如果用数学表达的话就是，$ c’=g(z)f(z_i)+cf\\big(z_f\\big) $，$ a=h(c^{\\prime})f(z_{0}) $。其中$ f $函数作为控制的阀门经常使用$ sigmod $激活函数，因为$ sigmod $的输出是在$ 0,1 $之间也就代表了允许进入多少，输出多少，保留多少。其中$ Forget\\ Gata $有一点和名字不同，当$ f(z_f) $的值越大代表保留的越多，和名字的意思有些相反。 相当于每个神经元都有了四个不同的输入，比之前的输入增加了四倍。 之前讲述的$ LSTM $还不是真正的$ LSTM $，实际上的应该将上次的存储值和隐藏层输出都作为这次的输入考虑进来。 在$ RNN $中还有一个需要注意的问题就是它的梯度可能会变化的非常剧烈，如果此时的梯度较大，但是我们的学习率也比较大，很可能出现$ loss $快速增大的情况，对于这种情况可以选择在梯度达到一定值的时候将其固定。 之所以会出现这种情况是因为随着迭代相同的操作可能会执行很多次，有的时候参数的改变起不到影响，但有的时候又会造成很大的改变。 而$ LSTM $可以解决一部分的问题，也就是避免梯度消失，但无法避免梯度爆炸，这样可以将学习率设置的小一些。之所以可以是因为$ LSTM $中始终保存了之前的结果，之前的过程对现在始终有影响，甚至一开始的$ LSTM $是没有$ Forget\\ Gate $这个设置的，而通常也将$ Forget\\ Gate $的值设置的很大，只有少数的情况会将原来的值重置。","link":"/2024/10/11/RNN/"},{"title":"Self-Attention","text":"引入 对于一个向量的输入计算我们很容易处理，但是现在我们希望对于一个句子的多个向量进行计算，这点可以体现在文字处理上。 将每个单词都作为一个向量处理，那么一个句子就是多个长度不一的向量合在一起。 对于多个向量的输入对应的输出可能分为三种情况，每个向量对应一个标签，全部向量对应一个标签，模型自己选择输出几个标签，今天主要处理的是第一种，即一个向量对应一个标签。 对于多个向量要想同时考虑到，可以使用一个$ window $，但是这不是最好的，因为这个窗口的大小是不固定，无法考虑到全部的向量，对于这种情况可以使用$ Self-Attention $。 Self-Attention ![](https://cdn.nlark.com/yuque/0/2024/png/40370285/1730627369098-d43ffdd6-8146-4600-832e-f0e3e58e5959.png) 我们可以多次使用$ self-attention $，然后配合上全连接网络使用，这个也被称为自注意力机制。 我们希望计算$ b^1 $的时候可以对于$ a^1 $同时考虑到$ a^2,a^3,a^4 $，因此我们需要知道他们彼此之间的关联性。 也就是使用一个$ \\alpha $来计算其中的关联性，对于这个$ \\alpha $的计算方法有很多种，这里提到了其中两种。 第一种是$ Dot-product $，先通过两个矩阵的变换再进行点乘，再$ self-attention $中使用的也是这个。第二种是$ Additive $，本次并没有使用。 首先使用刚才的$ Dot-product $计算$ a^1 $和它本身以及其余三个的关联性。 接着可以对输出的结果进行$ soft-max $，并不是一定要使用$ soft-max $，也可以使用$ relu $之类的方式。 最后使用进行变化的结果和向量$ v $相乘，最后将相乘的结果加在一起就得到了$ b^1 $。 刚才介绍了怎么得到$ b^1 $的方式，现在从矩阵乘法的角度重新介绍一下，可以看到将合在一起的矩阵$ I $乘上三个不同的矩阵，就可以得到$ Q,K,V $这三个矩阵。 使用得到的矩阵$ K $的转置和矩阵$ Q $相乘，就可以得到矩阵$ A $，通过$ soft-max $就可以得到$ {A’} $ 将得到的矩阵$ {A’} $和$ V $相乘就可以得到我们的输出向量$ O $，其实刚才的操作就是矩阵的相乘。 上图中从$ I $到$ O $的改变就是做的$ self-attention $的操作，其中就只有$ W^q,W^k,W^v $这三个矩阵参数是需要我们学习的。 多头自注意力机制 ![](https://cdn.nlark.com/yuque/0/2024/png/40370285/1730638668529-da28ab9b-07e1-425f-99ed-b2606da55413.png) 在计算相关性的时候可能不仅只有一种相关性，因此我们可以将$ q^i,k^i,v^i $的值都再乘不同的两个矩阵，最终可以得到两个$ b^{i,1},b^{i,2} $。 最后乘上一个$ W^o $的向量，可以得到一个$ b^i $的值。 刚才的计算中还存在一个问题，我们没有考虑向量之间的位置关系，不同的位置的影响没有被考虑进来，为了解决这个问题，我们可以计算每个位置对应的向量，将这个向量和$ a^i $相加进行计算从而将位置的信息也考虑进来。 CNN v.s. self-attention ![](https://cdn.nlark.com/yuque/0/2024/png/40370285/1730639423556-6dee23e1-22e8-4fd5-a8fe-affd10918c00.png) $ CNN $和$ self-attention $之间有很强的相关性，我们可以认为$ CNN $就是一个简化版的$ self-attention $，区别在于$ self-attention $更加的灵活，让机器自己去学习和自己相关的$ pixel $，而不是人为的设置感受野，对于$ self-attention $进行一些限制就可以做到$ CNN $一样的事。 既然$ self-attention $更加的灵活，那么也就代表更加容易过拟合，因此数据量很大的时候$ self-attention $的效果是要好于$ CNN $，但数据量不足的时候$ CNN $的效果就要比$ self-attention $要好一些。 RNN v.s. self-attention ![](https://cdn.nlark.com/yuque/0/2024/png/40370285/1730640096581-9284fd5a-1f61-4bfa-8c3d-cd6ddd21ea4b.png) 对于$ RNN $来讲，$ self-attention $虽然也都是处理多个向量的输入的，但是它并不是同时处理的，而是从左到右，在处理左边的时候无法考虑到右边，虽然也可以通过双向的$ RNN $完善，但是它也不是并行计算，因此现在更加倾向于使用$ self-attention $。 可参照： Transformer：注意力机制（attention）和自注意力机制（self-attention）的学习总结_注意力机制和自注意力机制-CSDN博客","link":"/2024/10/21/Self-Attention/"},{"title":"Transformer","text":"对于$Transformer$来讲，其中主要分为$Encoder$和$Decoder$，接下来首先介绍$Encoder$。 Encoder $Encoder$的主要作用是进行特征提取，这样做是因为原始输入中包含一些无用或干扰信息，这会使模型的性能和泛化性大打折扣。所以在这之前，我们通过$ Encoder $来先对数据进行一次特征提取和挖掘，比如后面会提到$ Encoder $里会有一个自注意力层，正如我们之前文章中提到，自注意力层可以挖掘输入内部元素直接的关系，这是模型直接接受原始输入很难做到。 $ Encoder $可以给定一排向量，输出一排同样的向量，对于这个要求$ RNN,CNN $都可以实现，而在$ Transformer $中主要使用的是$ self-attention $，但是这个模型有些复杂，接下来分步介绍这个$ Encoder $。 当向量输入之后要经过多个$ Block $，但是每个$ Block $并不是一层网络，而是一个$ self-attention $加上一个$ FC $。 但是这里的$ self-attention $跟之前的有些不同，还需要加上$ residual $和$ norm $，同时在$ FC $也加上这两个操作。需要注意的是这里的是层归一化。 层归一化和批量归一化： $ BatchNorm $把一个$ batch $中同一通道的所有特征（如下图红色区域）视为一个分布（有几个通道就有几个分布），并将其标准化。这意味着: 不同图片的的同一通道的相对关系是保留的，即不同图片的同一通道的特征是可以比较的 同一图片的不同通道的特征则是失去了可比性 $ LayerNorm $把一个样本的所有词义向量（如下图红色部分）视为一个分布（有几个句子就有几个分布），并将其标准化。这意味着: 同一句子中词义向量（上图中的$ V1, V2, …, VL $）的相对大小是保留的，或者也可以说$ LayerNorm $不改变词义向量的方向，只改变它的模。不同句子的词义向量则是失去了可比性。 用于$ NLP $领域解释: 考虑两个句子，“教练，我想打篮球！” 和 “老板，我要一打包子。”。通过比较两个句子中 “打” 的词义我们可以发现，词义并非客观存在的，而是由上下文的语义决定的。因此进行标准化时不应该破坏同一句子中不同词义向量的可比性，而$ LayerNorm $是满足这一点的，$ BatchNorm $则是不满足这一点的。且不同句子的词义特征也不应具有可比性，$ LayerNorm $也是能够把不同句子间的可比性消除。 $ Transformer $使用$ LayerNorm $而不是$ BatchNorm $的主要原因是$ LayerNorm $更加适合处理变长的序列数据。在$ Transformer $中，由于输入序列的长度是可变的，因此每个序列的批量统计信息（如均值和方差）也是不同的。而$ BatchNorm $是基于整个批量数据来计算统计信息的，这可能导致在处理变长序列时性能下降。相比之下，$ LayerNorm $是在每个样本的维度上进行归一化的，因此不受序列长度变化的影响。 在$ Transformer $中，$ LayerNorm $通常被放置在多头注意力机制和前馈神经网络的输出之后。通过对这些层的输出进行归一化，可以使得后续层的输入保持在一个相对稳定的范围内，有助于模型的训练。 简单对比如下： 参照于： 【深度学习中的批量归一化BN和层归一化LN】BN层（Batch Normalization）和LN层（Layer Normalization）的区别_bn和ln-CSDN博客 Transformer 系列三：Encoder编码器和Decoder解码器_transformer编码器-CSDN博客 对比一下刚刚看到的模型，只是将$ Self-Attention $换成了$ Mult-Head\\ Attention $，同时在输入之前加上了位置编码。 同时将全连接层换成了一个前馈神经网络，其实就是一个简单的两层全连接网络，用于进一步处理多头注意力机制的输出。这个网络首先通过一个线性变换将输入映射到一个更高维的空间中，然后通过一个非线性激活函数（如$ ReLU $）增加网络的非线性能力，最后再通过另一个线性变换将输出映射回原始维度。 在自注意力层后面增加前馈神经网络层的原因： 特征提取：$ FFN $通过两层全连接层（通常是线性层），对来自自注意力层的输出进行进一步的特征提取。这有助于模型学习到更深层次的、非线性的特征表示。 增加模型容量：通过引入额外的参数和非线性激活函数，$ FFN $增加了模型的容量，使得模型能够捕捉更复杂的数据模式和关系。 与自注意力机制互补：自注意力机制擅长捕捉序列内部的长距离依赖关系，而$ FFN $则专注于在给定的表示上进行特征转换。这种组合使得$ Transformer $能够有效地处理各种语言和序列任务。 提高泛化能力：$ FFN $通过增加模型的复杂性，有助于提高模型对未见数据的泛化能力。 其实$ Encoder $的架构并不是一定要这样设计，我们可以将其中的归一化的操作提前进行，这样的效果似乎看起来更好。 Decoder 我们刚才介绍了$ Encoder $，现在我们先不考虑$ Encoder $的输出怎么作为$ Decoder $的输入，现在假设已经可以传递。首先使用一个$ BEGIN $作为一个输入，同时根据这个输入得到一个输出，经过一个$ softmax $之后得到一个表，选择其中概率最大的值，将这个作为我们的预测结果，同时这个表的长度就是我们希望的词的范围大小，可能是中文的常用词，大概$ 3000,5000 $字左右，也可能是例如英文的常用词的大小。 接着使用上次的输出作为这次的输入，对于如果中间出现了错误是否会导致接下来的结果全部错误再下面会提到。 这个是$ Decoder $的结构图，看起来比$ Encoder $还要复杂，但是如果把中间的部分删除，其实别的部分基本一样。 可以看到，其余部分基本上一样，并没有什么很大的差别，只是其中的$ Multi-Head\\ Attention $换成了$ Masked \\ Multi-Head \\ Attention $。 对于$ Masked \\ Multi-Head \\ Attention $来讲，其中的主要区别就是，当$ q^2 $是，对应的$ k $只有$ k^1,k^2 $，只可以查询到当前和再前面的值，其实从刚才的$ Decoder $的原理也可以理解，其中的$ a^1,a^2,a^3,a^4 $并不是同时出现的，因此需要使用$ Masked $的形式。 刚才的$ Decoder $只有开始没有结束，为了可以让机器自己学习怎么结束，我们可以通过使用加上一个$ END $在表中的方式，其中当输出为$ END $的时候表示结束。 如图中所示，当输入“习”这个词的时候会输出一个$ END $表示结束。 现在对刚才讲的$ Decoder $做一个总结，刚才讲的$ Decoder $全是$ AT\\ Eecoder $的形式，但是还有一种$ NAT $的形式，也就是不再一个一个输入，而是采用一次性同时输入的形式，但是这样需要采用一定的措施，例如使用一个新的预测将会有几个输出，或者直接输入很多的$ BEGIN $，然后根据$ END $截断，只要$ END $前面的。 现在需要介绍$ Decoder $是怎么使用$ Encoder $的信息的，通过经过$ Masked \\ Multi-Head \\ Attention $生成的向量得到一个$ q $，然后使用这个$ q $和$ Encoder $计算得到一个$ v $作为接下来全连接层的输入。 而且未必一定要用$ Encoder $的最后一层的输出作为输入，还可以使用别的方式进行信息的传递。 Training 刚才介绍的都是我们已经训练好了一个模型，然后测试这个模型。但是现在要介绍的是怎么训练一个模型，首先我们需要人为的加上标签，也就是使用$ one-hot $表示，同时最小化输出和标签之间的$ cross \\ entropy $，同时需要注意的是刚才提的到那个问题，如果在$ Decoder $中预测数据不正确怎么办，因此我们需要使用$ Ground\\ Truth $，也就是正确的数据作为输入。 刚才的方法还存在一个曝光偏差($ exposure \\ bias $)，训练($ training $) 时接受的标签是真实的值($ ground\\ truth\\ input $)，但测试 ($ testing $) 时却接受自己前一个单元的输出($ output $)作为本单元的输入($ input $)，这两个$ setting $不一致会导致误差累积。 解决这个问题也不麻烦，只要在训练的时候在正确的标签中故意加入一些错误的数据即可。","link":"/2024/11/11/Transformer/"},{"title":"test","text":"","link":"/2024/11/11/test-1/"},{"title":"test","text":"","link":"/2024/11/11/test/"},{"title":"卷积神经网络","text":"对于卷积神经网络来讲，卷积，下采样，全连接是属于卷积神经网络最重要三个部分，接下来分别进行介绍。 在之前的全连接中，在处理数据之前我们通常将全部的数据拉伸为一个直线，接着进行数据处理，但是这样的话会丢失一些空间信息。 因此，我们就需要使用这个卷积神经网络，每个卷积层跟着一个池化，最终加上全连接和输出层。 上图就是具体的卷积过程，其中$ input $中的绿色部分就是一个局部感受野，中间的$ 3\\times3 $的矩阵就是一个卷积核，将这两个内容两两相乘再相加，最终得到的数据就是输出的值。 每个卷积核经过计算之后都会得到一个$ feature\\ map $（特征图），上图中存在三个卷积核，因此得到三个$ feature\\ map $，将这三个$ feature\\ map $叠加在一起就是下一层的输入值。 同时需要注意的是，在计算中边缘数据的访问要少于中间数据的访问，因此我们可以通过填充数据$ 0 $来进行优化，这样也可以避免随着卷积计算得到的$ feature\\ map $越来越小。 还需要注意的是，对于一个多通道的输入，对应的卷积核也要有相应的通道数，也就是对于一个三通道的输入，它的卷积核也要是三通道的。 对应上图，也就是第二个卷积核正在生成第二个$ feature\\ map $，有多少个卷积核就生成多少个$ feature\\ map $，最终将这些$ feature\\ map $摞在一起。 这个图和上一个相同，都是对于一个多通道的输入的一个处理，还可加上一个偏置项。 其实每一次的卷积操作都可以看成是一个全连接的神经元。 刚刚已经学习了卷积操作，但是还需要知道为什么需要进行卷积操作，卷积的意义就是将原图上符合卷积核特征的特征提取出来，例如上图中我们的卷积核就是一个眼睛，那么原图中眼睛的部分数值最高， 也就是得到这样的卷积输出，提取出眼睛的特征。 可以看到，有多少个卷积核那么久有多少个$ feature\\ map $，那么如果具有$ 256 $个卷积核得到的$ feature\\ map $就有$ 256 $，这样得到数据是很大的，因此我们可以选择其中的一部分，也就是用到了池化操作，也叫做下采样。 池化的操作有些类似于前面的卷积，但是没有卷积核，可以通过使用最大池化或者平均池化。 可以看到，即使原图发生了平移，但是我们提取到的池化之后的结果确实相同的，因此池化操作是具有平移不变性的，因此在识别图片时即使发生了偏移，伸缩也不会影响。 在池化中，除了刚刚提到的减少参数和平移不变性，还具有防止过拟合的性质，因为是提取了一个方块中最重要的信息，因此可以避免一些噪音点的干扰，可以避免过拟合。 在这幅图中最上面可以看到卷积的过程，经过卷积，池化，最终全连接并使用$ softmax $输出。 由于同一个卷积核在计算时是不变，因此说具有权重共享的性质。 对于如何计算一个$ feature\\ map $的大小，可以通过$ (N-F)/stride+1 $这个计算公式进行计算，同时需要避免出现步长为小数的情况。 如果算上填充$ 0 $的情况就是加上双倍的$ padding $的大小，同时计算可得，当$ p=(F-1)/2 $时，输入和输出的长宽相等。 总结一下，通过上图计算我们可以得到输出的$ feature\\ map $的大小和总权重的个数。 再试想一下，如果是一个$ 1\\times1 $的卷积核，它具有什么作用。 可以看到，这个卷积核具有升维或者降维的作用，当$ n $大于$ 32 $时就是升维，相反就是降维。 对于普通的卷积神经网络，计算的运算量是一个很大的数量级，但是如果可以通过$ 1\\times1 $的卷积核进行降维，可以减少参数量，同时将不同通道之间的数据进行交流，加上一层降维操作也等同于增加模型深度，提高了非线性的能力。 每一个卷积核也相当于一个神经元的全连接操作，其中原图上的元素相当于$ x $，卷积核上的权重相当于$ w $，一次卷积操作也相当于一个全连接操作。 对于图中的五个通道值，是五个不同的卷积核对于同一个感受野的计算值累加在一起，因此一个$ 1\\times1 $的卷积核相当于对于不同通道的值穿在一起。","link":"/2024/07/11/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","link":"/tags/DeepLearning/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"}],"categories":[{"name":"blog","slug":"blog","link":"/categories/blog/"}],"pages":[{"title":"about","text":"🙋 Hello there 👋 I am a junior software engineering student with knowledge in Java backend development and currently learning about machine learning. 💼 Student at Zhengzhou Light Industry University. 🌱 Currently learning Machine Learning, Java, Operating Systems, Computer Networks, Data Structures, Algorithms, Principles of Computer Composition. 📚 Reading Computer Networks, Seventh Edition. 💻 Two or more years of computer science literacy studies. ⛵ Encouraging open-source collaborations. ✍🏻 Writing about Programming &amp; Tech on Personal Blog. 👑 GitHub statistical reports: Take a look at my repositories and let's get in touch!","link":"/about/index.html"}]}