{"posts":[{"title":"RNN","text":"介绍 ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181423393.png) 对用同一个词语在不同的话中可能具有完全相反的意思，但是如果还是使用之前神经网络，那么输入结果一定是相同的，为了解决这个问题，我们希望神经网络可以具有一些记忆性，这就是接下来要介绍的循环神经网络。 因此我们讲隐藏层的输出结果全部都放入到一个$ store $中进行存储，在下次输入中再取出来进行使用。 用一个例子解释，当输入为$ [1,1] $，权重为$ 1 $，无偏置的情况，第一次的输出就是$ [4,4] $，同时隐藏层的$ [2,2] $存储在$ store $中。 在第二次中，使用了上一次的存储值，因此就是相当于有四个输入$ 1,1,2,2 $因此最终即使输入相同，但是输出也不同。 需要注意的是，输入的顺序不同，那么得到的结果也会有所不同。 刚才讲的是$ Elman\\ Network $，即存储隐藏层的数据，还可以像$ jordan\\ Network $一样存储上一次的输出层的结果。 除了刚才的神经网络，还有别的变形，例如上图的双向循环神经网络。 LSTM ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181813125.png) 除了像刚刚讲到的最基本的$ RNN $，还可以对它的存储内容进行调整。使用$ Input\\ Gate $作为输入控制，判断是否可以讲结果存储$ store $中，使用$ Forget\\ Gate $作为判断是否要将存储的数据清空，使用$ Output\\ Gate $作为输出控制，判断别的网络是否可以使用这个存储的结果。 有一个冷知识，这个名字长短时记忆网络的断句是长 短时记忆网络，是较长的短时记忆网络，因为比最原始的$ RNN $保留的时间长一些。 刚才提到的思路如果用数学表达的话就是，$ c’=g(z)f(z_i)+cf\\big(z_f\\big) $，$ a=h(c^{\\prime})f(z_{0}) $。其中$ f $函数作为控制的阀门经常使用$ sigmod $激活函数，因为$ sigmod $的输出是在$ 0,1 $之间也就代表了允许进入多少，输出多少，保留多少。其中$ Forget\\ Gata $有一点和名字不同，当$ f(z_f) $的值越大代表保留的越多，和名字的意思有些相反。 相当于每个神经元都有了四个不同的输入，比之前的输入增加了四倍。 之前讲述的$ LSTM $还不是真正的$ LSTM $，实际上的应该将上次的存储值和隐藏层输出都作为这次的输入考虑进来。 在$ RNN $中还有一个需要注意的问题就是它的梯度可能会变化的非常剧烈，如果此时的梯度较大，但是我们的学习率也比较大，很可能出现$ loss $快速增大的情况，对于这种情况可以选择在梯度达到一定值的时候将其固定。 之所以会出现这种情况是因为随着迭代相同的操作可能会执行很多次，有的时候参数的改变起不到影响，但有的时候又会造成很大的改变。 而$ LSTM $可以解决一部分的问题，也就是避免梯度消失，但无法避免梯度爆炸，这样可以将学习率设置的小一些。之所以可以是因为$ LSTM $中始终保存了之前的结果，之前的过程对现在始终有影响，甚至一开始的$ LSTM $是没有$ Forget\\ Gate $这个设置的，而通常也将$ Forget\\ Gate $的值设置的很大，只有少数的情况会将原来的值重置。","link":"/2024/10/11/RNN/"},{"title":"Transformer","text":"对于$Transformer$来讲，其中主要分为$Encoder$和$Decoder$，接下来首先介绍$Encoder$。 Encoder $Encoder$的主要作用是进行特征提取，这样做是因为原始输入中包含一些无用或干扰信息，这会使模型的性能和泛化性大打折扣。所以在这之前，我们通过$ Encoder $来先对数据进行一次特征提取和挖掘，比如后面会提到$ Encoder $里会有一个自注意力层，正如我们之前文章中提到，自注意力层可以挖掘输入内部元素直接的关系，这是模型直接接受原始输入很难做到。 $ Encoder $可以给定一排向量，输出一排同样的向量，对于这个要求$ RNN,CNN $都可以实现，而在$ Transformer $中主要使用的是$ self-attention $，但是这个模型有些复杂，接下来分步介绍这个$ Encoder $。 当向量输入之后要经过多个$ Block $，但是每个$ Block $并不是一层网络，而是一个$ self-attention $加上一个$ FC $。 但是这里的$ self-attention $跟之前的有些不同，还需要加上$ residual $和$ norm $，同时在$ FC $也加上这两个操作。需要注意的是这里的是层归一化。 层归一化和批量归一化： $ BatchNorm $把一个$ batch $中同一通道的所有特征（如下图红色区域）视为一个分布（有几个通道就有几个分布），并将其标准化。这意味着: 不同图片的的同一通道的相对关系是保留的，即不同图片的同一通道的特征是可以比较的 同一图片的不同通道的特征则是失去了可比性 $ LayerNorm $把一个样本的所有词义向量（如下图红色部分）视为一个分布（有几个句子就有几个分布），并将其标准化。这意味着: 同一句子中词义向量（上图中的$ V1, V2, …, VL $）的相对大小是保留的，或者也可以说$ LayerNorm $不改变词义向量的方向，只改变它的模。不同句子的词义向量则是失去了可比性。 用于$ NLP $领域解释: 考虑两个句子，“教练，我想打篮球！” 和 “老板，我要一打包子。”。通过比较两个句子中 “打” 的词义我们可以发现，词义并非客观存在的，而是由上下文的语义决定的。因此进行标准化时不应该破坏同一句子中不同词义向量的可比性，而$ LayerNorm $是满足这一点的，$ BatchNorm $则是不满足这一点的。且不同句子的词义特征也不应具有可比性，$ LayerNorm $也是能够把不同句子间的可比性消除。 $ Transformer $使用$ LayerNorm $而不是$ BatchNorm $的主要原因是$ LayerNorm $更加适合处理变长的序列数据。在$ Transformer $中，由于输入序列的长度是可变的，因此每个序列的批量统计信息（如均值和方差）也是不同的。而$ BatchNorm $是基于整个批量数据来计算统计信息的，这可能导致在处理变长序列时性能下降。相比之下，$ LayerNorm $是在每个样本的维度上进行归一化的，因此不受序列长度变化的影响。 在$ Transformer $中，$ LayerNorm $通常被放置在多头注意力机制和前馈神经网络的输出之后。通过对这些层的输出进行归一化，可以使得后续层的输入保持在一个相对稳定的范围内，有助于模型的训练。 简单对比如下： 参照于： 【深度学习中的批量归一化BN和层归一化LN】BN层（Batch Normalization）和LN层（Layer Normalization）的区别_bn和ln-CSDN博客 Transformer 系列三：Encoder编码器和Decoder解码器_transformer编码器-CSDN博客 对比一下刚刚看到的模型，只是将$ Self-Attention $换成了$ Mult-Head\\ Attention $，同时在输入之前加上了位置编码。 同时将全连接层换成了一个前馈神经网络，其实就是一个简单的两层全连接网络，用于进一步处理多头注意力机制的输出。这个网络首先通过一个线性变换将输入映射到一个更高维的空间中，然后通过一个非线性激活函数（如$ ReLU $）增加网络的非线性能力，最后再通过另一个线性变换将输出映射回原始维度。 在自注意力层后面增加前馈神经网络层的原因： 特征提取：$ FFN $通过两层全连接层（通常是线性层），对来自自注意力层的输出进行进一步的特征提取。这有助于模型学习到更深层次的、非线性的特征表示。 增加模型容量：通过引入额外的参数和非线性激活函数，$ FFN $增加了模型的容量，使得模型能够捕捉更复杂的数据模式和关系。 与自注意力机制互补：自注意力机制擅长捕捉序列内部的长距离依赖关系，而$ FFN $则专注于在给定的表示上进行特征转换。这种组合使得$ Transformer $能够有效地处理各种语言和序列任务。 提高泛化能力：$ FFN $通过增加模型的复杂性，有助于提高模型对未见数据的泛化能力。 其实$ Encoder $的架构并不是一定要这样设计，我们可以将其中的归一化的操作提前进行，这样的效果似乎看起来更好。 Decoder 我们刚才介绍了$ Encoder $，现在我们先不考虑$ Encoder $的输出怎么作为$ Decoder $的输入，现在假设已经可以传递。首先使用一个$ BEGIN $作为一个输入，同时根据这个输入得到一个输出，经过一个$ softmax $之后得到一个表，选择其中概率最大的值，将这个作为我们的预测结果，同时这个表的长度就是我们希望的词的范围大小，可能是中文的常用词，大概$ 3000,5000 $字左右，也可能是例如英文的常用词的大小。 接着使用上次的输出作为这次的输入，对于如果中间出现了错误是否会导致接下来的结果全部错误再下面会提到。 这个是$ Decoder $的结构图，看起来比$ Encoder $还要复杂，但是如果把中间的部分删除，其实别的部分基本一样。 可以看到，其余部分基本上一样，并没有什么很大的差别，只是其中的$ Multi-Head\\ Attention $换成了$ Masked \\ Multi-Head \\ Attention $。 对于$ Masked \\ Multi-Head \\ Attention $来讲，其中的主要区别就是，当$ q^2 $是，对应的$ k $只有$ k^1,k^2 $，只可以查询到当前和再前面的值，其实从刚才的$ Decoder $的原理也可以理解，其中的$ a^1,a^2,a^3,a^4 $并不是同时出现的，因此需要使用$ Masked $的形式。 刚才的$ Decoder $只有开始没有结束，为了可以让机器自己学习怎么结束，我们可以通过使用加上一个$ END $在表中的方式，其中当输出为$ END $的时候表示结束。 如图中所示，当输入“习”这个词的时候会输出一个$ END $表示结束。 现在对刚才讲的$ Decoder $做一个总结，刚才讲的$ Decoder $全是$ AT\\ Eecoder $的形式，但是还有一种$ NAT $的形式，也就是不再一个一个输入，而是采用一次性同时输入的形式，但是这样需要采用一定的措施，例如使用一个新的预测将会有几个输出，或者直接输入很多的$ BEGIN $，然后根据$ END $截断，只要$ END $前面的。 现在需要介绍$ Decoder $是怎么使用$ Encoder $的信息的，通过经过$ Masked \\ Multi-Head \\ Attention $生成的向量得到一个$ q $，然后使用这个$ q $和$ Encoder $计算得到一个$ v $作为接下来全连接层的输入。 而且未必一定要用$ Encoder $的最后一层的输出作为输入，还可以使用别的方式进行信息的传递。 Training 刚才介绍的都是我们已经训练好了一个模型，然后测试这个模型。但是现在要介绍的是怎么训练一个模型，首先我们需要人为的加上标签，也就是使用$ one-hot $表示，同时最小化输出和标签之间的$ cross \\ entropy $，同时需要注意的是刚才提的到那个问题，如果在$ Decoder $中预测数据不正确怎么办，因此我们需要使用$ Ground\\ Truth $，也就是正确的数据作为输入。 刚才的方法还存在一个曝光偏差($ exposure \\ bias $)，训练($ training $) 时接受的标签是真实的值($ ground\\ truth\\ input $)，但测试 ($ testing $) 时却接受自己前一个单元的输出($ output $)作为本单元的输入($ input $)，这两个$ setting $不一致会导致误差累积。 解决这个问题也不麻烦，只要在训练的时候在正确的标签中故意加入一些错误的数据即可。","link":"/2024/11/11/Transformer/"},{"title":"Self-Attention","text":"引入 对于一个向量的输入计算我们很容易处理，但是现在我们希望对于一个句子的多个向量进行计算，这点可以体现在文字处理上。 将每个单词都作为一个向量处理，那么一个句子就是多个长度不一的向量合在一起。 对于多个向量的输入对应的输出可能分为三种情况，每个向量对应一个标签，全部向量对应一个标签，模型自己选择输出几个标签，今天主要处理的是第一种，即一个向量对应一个标签。 对于多个向量要想同时考虑到，可以使用一个$ window $，但是这不是最好的，因为这个窗口的大小是不固定，无法考虑到全部的向量，对于这种情况可以使用$ Self-Attention $。 Self-Attention 我们可以多次使用$ self-attention $，然后配合上全连接网络使用，这个也被称为自注意力机制。 我们希望计算$ b^1 $的时候可以对于$ a^1 $同时考虑到$ a^2,a^3,a^4 $，因此我们需要知道他们彼此之间的关联性。 也就是使用一个$ \\alpha $来计算其中的关联性，对于这个$ \\alpha $的计算方法有很多种，这里提到了其中两种。 第一种是$ Dot-product $，先通过两个矩阵的变换再进行点乘，再$ self-attention $中使用的也是这个。第二种是$ Additive $，本次并没有使用。 首先使用刚才的$ Dot-product $计算$ a^1 $和它本身以及其余三个的关联性。 接着可以对输出的结果进行$ soft-max $，并不是一定要使用$ soft-max $，也可以使用$ relu $之类的方式。 最后使用进行变化的结果和向量$ v $相乘，最后将相乘的结果加在一起就得到了$ b^1 $。 刚才介绍了怎么得到$ b^1 $的方式，现在从矩阵乘法的角度重新介绍一下，可以看到将合在一起的矩阵$ I $乘上三个不同的矩阵，就可以得到$ Q,K,V $这三个矩阵。 使用得到的矩阵$ K $的转置和矩阵$ Q $相乘，就可以得到矩阵$ A $，通过$ soft-max $就可以得到$ {A’} $ 将得到的矩阵$ {A’} $和$ V $相乘就可以得到我们的输出向量$ O $，其实刚才的操作就是矩阵的相乘。 上图中从$ I $到$ O $的改变就是做的$ self-attention $的操作，其中就只有$ W^q,W^k,W^v $这三个矩阵参数是需要我们学习的。 多头自注意力机制 在计算相关性的时候可能不仅只有一种相关性，因此我们可以将$ q^i,k^i,v^i $的值都再乘不同的两个矩阵，最终可以得到两个$ b^{i,1},b^{i,2} $。 最后乘上一个$ W^o $的向量，可以得到一个$ b^i $的值。 刚才的计算中还存在一个问题，我们没有考虑向量之间的位置关系，不同的位置的影响没有被考虑进来，为了解决这个问题，我们可以计算每个位置对应的向量，将这个向量和$ a^i $相加进行计算从而将位置的信息也考虑进来。 CNN v.s. self-attention $ CNN $和$ self-attention $之间有很强的相关性，我们可以认为$ CNN $就是一个简化版的$ self-attention $，区别在于$ self-attention $更加的灵活，让机器自己去学习和自己相关的$ pixel $，而不是人为的设置感受野，对于$ self-attention $进行一些限制就可以做到$ CNN $一样的事。 既然$ self-attention $更加的灵活，那么也就代表更加容易过拟合，因此数据量很大的时候$ self-attention $的效果是要好于$ CNN $，但数据量不足的时候$ CNN $的效果就要比$ self-attention $要好一些。 RNN v.s. self-attention 对于$ RNN $来讲，$ self-attention $虽然也都是处理多个向量的输入的，但是它并不是同时处理的，而是从左到右，在处理左边的时候无法考虑到右边，虽然也可以通过双向的$ RNN $完善，但是它也不是并行计算，因此现在更加倾向于使用$ self-attention $。 可参照： Transformer：注意力机制（attention）和自注意力机制（self-attention）的学习总结_注意力机制和自注意力机制-CSDN博客","link":"/2024/10/13/Self-Attention/"},{"title":"卷积神经网络","text":"对于卷积神经网络来讲，卷积，下采样，全连接是属于卷积神经网络最重要三个部分，接下来分别进行介绍。 在之前的全连接中，在处理数据之前我们通常将全部的数据拉伸为一个直线，接着进行数据处理，但是这样的话会丢失一些空间信息。 因此，我们就需要使用这个卷积神经网络，每个卷积层跟着一个池化，最终加上全连接和输出层。 上图就是具体的卷积过程，其中$ input $中的绿色部分就是一个局部感受野，中间的$ 3\\times3 $的矩阵就是一个卷积核，将这两个内容两两相乘再相加，最终得到的数据就是输出的值。 每个卷积核经过计算之后都会得到一个$ feature\\ map $（特征图），上图中存在三个卷积核，因此得到三个$ feature\\ map $，将这三个$ feature\\ map $叠加在一起就是下一层的输入值。 同时需要注意的是，在计算中边缘数据的访问要少于中间数据的访问，因此我们可以通过填充数据$ 0 $来进行优化，这样也可以避免随着卷积计算得到的$ feature\\ map $越来越小。 还需要注意的是，对于一个多通道的输入，对应的卷积核也要有相应的通道数，也就是对于一个三通道的输入，它的卷积核也要是三通道的。 对应上图，也就是第二个卷积核正在生成第二个$ feature\\ map $，有多少个卷积核就生成多少个$ feature\\ map $，最终将这些$ feature\\ map $摞在一起。 这个图和上一个相同，都是对于一个多通道的输入的一个处理，还可加上一个偏置项。 其实每一次的卷积操作都可以看成是一个全连接的神经元。 刚刚已经学习了卷积操作，但是还需要知道为什么需要进行卷积操作，卷积的意义就是将原图上符合卷积核特征的特征提取出来，例如上图中我们的卷积核就是一个眼睛，那么原图中眼睛的部分数值最高， 也就是得到这样的卷积输出，提取出眼睛的特征。 可以看到，有多少个卷积核那么久有多少个$ feature\\ map $，那么如果具有$ 256 $个卷积核得到的$ feature\\ map $就有$ 256 $，这样得到数据是很大的，因此我们可以选择其中的一部分，也就是用到了池化操作，也叫做下采样。 池化的操作有些类似于前面的卷积，但是没有卷积核，可以通过使用最大池化或者平均池化。 可以看到，即使原图发生了平移，但是我们提取到的池化之后的结果确实相同的，因此池化操作是具有平移不变性的，因此在识别图片时即使发生了偏移，伸缩也不会影响。 在池化中，除了刚刚提到的减少参数和平移不变性，还具有防止过拟合的性质，因为是提取了一个方块中最重要的信息，因此可以避免一些噪音点的干扰，可以避免过拟合。 在这幅图中最上面可以看到卷积的过程，经过卷积，池化，最终全连接并使用$ softmax $输出。 由于同一个卷积核在计算时是不变，因此说具有权重共享的性质。 对于如何计算一个$ feature\\ map $的大小，可以通过$ (N-F)/stride+1 $这个计算公式进行计算，同时需要避免出现步长为小数的情况。 如果算上填充$ 0 $的情况就是加上双倍的$ padding $的大小，同时计算可得，当$ p=(F-1)/2 $时，输入和输出的长宽相等。 总结一下，通过上图计算我们可以得到输出的$ feature\\ map $的大小和总权重的个数。 再试想一下，如果是一个$ 1\\times1 $的卷积核，它具有什么作用。 可以看到，这个卷积核具有升维或者降维的作用，当$ n $大于$ 32 $时就是升维，相反就是降维。 对于普通的卷积神经网络，计算的运算量是一个很大的数量级，但是如果可以通过$ 1\\times1 $的卷积核进行降维，可以减少参数量，同时将不同通道之间的数据进行交流，加上一层降维操作也等同于增加模型深度，提高了非线性的能力。 每一个卷积核也相当于一个神经元的全连接操作，其中原图上的元素相当于$ x $，卷积核上的权重相当于$ w $，一次卷积操作也相当于一个全连接操作。 对于图中的五个通道值，是五个不同的卷积核对于同一个感受野的计算值累加在一起，因此一个$ 1\\times1 $的卷积核相当于对于不同通道的值穿在一起。","link":"/2024/07/11/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"图像分类:KNN和线性分类器","text":"K-Nearest Neighbors算法 在$ cs231n $的课程中，首先介绍了$ k $近邻算法，但由于课件上没有介绍$ KD $树，因此采用另外一个课程的讲解。 第3章 k-近邻算法_哔哩哔哩_bilibili 首先，$ k $近邻算法的原理就在于找到离自己最近的$ k $个点，对于分类问题，采用多数表决法，对于回归问题，计算平均值，这里有些类似于决策树。 例如对于这个图像来讲，我们选择$ k $为$ 3 $，也就是离这个点最近的三个点，其中两个红三角，一个蓝方块，因此我们将其预测为红三角。 对于$ k $近邻算法分为这几个步骤，可以看到我们基本上没有学习的过程，相比于之前学到的逻辑回归，我们的训练过程仅仅是将所有的数据读入，因此$ k $近邻算法也被称为是一种懒散的学习方法，基本上不学习。同时我们也看到在$ k $近邻算法中，$ k $值的选取是十分重要的，接下来我们将介绍这个部分。 可以看到，对于不同的$ k $来讲，我们随着$ k $的增大，那么这个分类也就变得更加平滑，但是同时误差也会增大，如果使用一个极端的情况，$ k $等于样本个数，那么就没有分类，样本中哪个多就属于哪个，当$ k $太小时也不好，这样虽然误差减小，但是模型会过于复杂，很容易过拟合，因此，我们需要通过交叉验证选择一个合适的超参数$ k $。 除了$ k $的值，我们还需要选择怎么衡量这个距离，其中我们经常使用曼哈顿距离和欧式距离。 KD树 ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181818958.png) 我们刚刚的计算方法也就是暴力的计算方法，但是在这种方法下，虽然我们的训练时间很短，仅需要将全部的数据读入即可，但是在测试中，对于每个测试数据我们都需要遍历全部的数据，如果这个数据集很大的话，我们的测试时间也会变得很大，为了解决这个问题，我们可以使用数据结构中的树。 $ KD $树做作为一种二叉树，可以有效地划分数据集，避免对于全部的数据都进行访问。 首先，构建一个$ KD $树。我们先选择一个维度进行划分，我在不同的资料中找到两种划分的方式，第一种是视频中的这种方式，先选择$ x $轴，接着循环划分，如果有$ y,z $的话就一直循环，第二种是刘建平老师的博客中讲到，选择方差最大的维度作为划分，在本文中选择第一种方式，比较简单。 对于这个图，首先，我们以$ x $轴作为划分，因此，我们需要选择其中的中位数，这个例子中由于存在偶数个数，因此，我们可以选择$ (5,4) $也可以选择$ (7,2) $，我们选择较大的$ (7,2) $，接着，将$ x $轴上小于$ 7 $的$ (5,4),(4,7),(2,3) $划入左子树，将$ (9,6),(8,1) $划为右子树。接着在选择$ y $轴作为划分，最终可以得到下图。 图中在第三层就划分完成了，因此，如果还有数据的话重新选择以$ x $轴划分。 现在通过构建好的$ KD $树，介绍$ KD $树搜索。 当我们生成KD树以后，就可以去预测测试集里面的样本目标点了。对于一个目标点，我们首先在KD树里面找到包含目标点的叶子节点。以目标点为圆心，以目标点到叶子节点样本实例的距离为半径，得到一个超球体，最近邻的点一定在这个超球体内部。然后返回叶子节点的父节点，检查另一个子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点寻找是否有更加近的近邻,有的话就更新最近邻。如果不相交那就简单了，我们直接返回父节点的父节点，在另一个子树继续搜索最近邻。当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。 虽然文字描述很绕口，但实际上的思路很简单，首先我们找到一个临时的最近的节点，也就是找到一个叶子节点，类似于二分搜索树，我们根据大于小于当前节点进行划分。 如图，$ (2,4.5) $在$ x $轴上，小于$ 7 $，因此，划分到左子树，又因为在$ y $轴上大于$ 4 $，因此划分到右子树，最终到达叶子节点$ (4,7) $。 但是这仅仅是一个临时的最小值，我们可以看到$ (2,4.5) $虽然是属于$ (4,7) $所在的范围，但是由于这个范围太大，而这两个点离得并不是很近，反而是$ (2,3) $和$ (2,4.5) $离得更近一些。 首先我们计算节点$ (4,7) $，可以看到，这个节点不小于我们的临时最小值，接着判断另一个兄弟节点$ (2,3) $所在的范围是否与以$ (2,4.5) $为球心，$ (2,4.5),(4,7) $之间距离为半径的超球体有相交的地方，虽然这个图我画的不是很标准，但是可以明显看到是存在一个相交的，因此我们需要将其移动到$ (2,3) $，经过计算，这个距离小于$ (4,7) $，因此将其更新为临时的最小距离，这个节点的兄弟节点已经判断过了，因此我们回到这个节点的父节点$ (5,4) $。 由于这个节点的距离不小于我们的临时最小值，因此不需要改变，同时这个新的超球体和当前节点$ (5,4) $的兄弟节点$ (9,6) $所在的范围也并不相交，因此不需要递归这个右子树，关于递归这个可以参照这篇博客的详细介绍，实际上就是重新按照第一步找到这个子树下的叶子节点。 kd tree最近邻搜索算法深度解析_kdtree最近邻算法-CSDN博客 接着回到根节点，计算这个距离不小于临时最小距离，因此搜索结束，则$ (2,3) $为最小距离节点。 通过了解了$ KD $树搜索最小距离，在计算$ k $个值时，仅需要屏蔽已经标记的最小距离重新进行循环计算$ k $次即可，同时，如果数据量不太，我们还可以通过维护一个小根堆来遍历一个即可。在使用$ KD $树中由于我们减少对于一些无意义的遍历，缩短了测试时间，但是数据分布不是很均匀的时候，这种方法的效果就不是很好了，对于这种情况可以参照这篇博客。 K近邻法(KNN)原理小结 - 刘建平Pinard - 博客园 我们虽然提到了$ KNN $算法，但是在实际的图像分类中我们并不会使用这个方法，除了这个方法在测试的时候太慢，还会出现例如上图中右侧三个图片虽然不一样，但是如果计算他们和左侧图片的$ L2 $距离却会出现一样值的结果。 同时当我们的维度提到了，需要的数据则是一个指数的增长，这些问题都导致我们在实际上并不会使用这个方法。 线性分类器 除了$ KNN $，我们还有一种分类方法，称为线性分类器，接下来从三个角度理解这个分类器。 代数角度 假设当前有一个长$ 32 $，宽$ 32 $，通道为$ 3 $的图像，将这个图像转换为一个列向量，也就是一个$ 3072\\times1 $的列向量。同时我们将其分为$ 10 $类，对于每一个我们都需要$ 3072 $个权重，因此我们就得到了一个$ 10\\times3072 $的矩阵，使用这个权重矩阵乘上像素值加上截距，也就得到了我们的分类结果。 使用这个例子，我们猫的图片有四个像素点，因此就伸长为一个$ 4\\times1 $的列向量，同时因为需要划分为$ 3 $类，因此需要一个$ 3\\times4 $的权重矩阵，最后加上偏置即可。 直观角度 从直观角度上来讲就是讲训练的$ 3072 $个权重变回原本的$ 32\\times32\\times3 $的矩阵，可以看到对于飞机来讲蓝色像素点的占比分高，也就是讲如果有一个图片跟这个图片很接近，那么高的权重乘上高的像素自然结果也就越高，说明也就越是飞机。 几何角度 从几何上来讲，这个线性分类在一维上就是点，二维上就是直线，三维上就是面，更高维上就是超平面，例如上图中划分了$ 10 $个超平面。 同时线性分类也具有一些问题，对于线性不可分的情况，例如图一的异或问题，图二的环状问题，我们的线性分类都是无法满足的。","link":"/2024/09/17/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%EF%BC%9AKNN%E5%92%8C%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/"},{"title":"损失函数和优化","text":"SVM分类 ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181819441.png) 上一节已经介绍了线性分类器，但是我们并没有对其进行评估。现在我们需要对其进行评估，首先定义一个损失函数，接着然后对其进行优化，使其$ loss $最小化。 首先使用$ SVM $分类，对于这个算法在这里不多介绍，具体在机器学习中详细介绍，这里直接使用它的损失函数，也就是前面提到的合页函数，但是需要注意的是，这里的损失函数并不是前面直接使用目标值和预测值的乘积，在实际应用的时候，一方面，预测值$ y $并不总是属于$ [-1,1] $，也可能属于其他的取值范围；另一方面，很多时候我们希望训练的是两个元素之间的相似关系，而非样本的类别得分，因此我们更倾向于使用二者的差值预测二者之间的相似关系。 当我们对于猫这张图片进行计算的时候，可以看到这个损失函数的值就是$ 2.9 $ 可以看到即使两个$ w $的值不同，但只要是关系，那么损失值也是相同的，那么如何衡量使用合适的权重，这就需要使用正则化。 图中展示了我们进行使用的$ L1,L2 $正则化，以及$ Dropout $等避免过拟合的方法。 softmax分类 ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181819195.png) 除了上一节的$ SVM $，对于分类问题，例如对于多分类的逻辑回归问题，我们还可以使用$ softmax $激活函数，因此可以计算出不同类型的概率，还可以使用最大似然估计求得损失函数，这一部分也是属于机器学习的内容，这里不过多阐述。","link":"/2024/08/17/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96/"},{"title":"训练神经网络","text":"在选择数据时，我们可以选择一次性使用全部的数据进行训练，但是这样的计算量太大，还可以选择每次选择一个数据进行计算，但是这样容易受到噪音的干扰，因此我们希望采用一种更加折中的方式，即采用小批量随机梯度下降法。 这样既没有那么震荡，也没有那么大的计算量。 激活函数选择 sigmod ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181814349.png) 梯度消失问题 可以看到当$ x $过大或过小时，很容易造成梯度消失的问题，这不利于对于参数的调整。 不关于$ 0 $对称问题 指数运算问题 在进行指数运算时容易产生很大的计算开销。 tanh ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181814012.png) $ \\tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}} $，实际上是关于$ sigmod $函数的变型，也就是$ tanh(x)=2sigmoid(2x)-1 $。 避免了不关于原点对称的$ zig-zag $问题，但仍存在梯度消失问题。 ReLU ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181814901.png) 关于$ ReLU $激活函数具有不饱和，计算快，收敛快这一系列优点，但是它仍具有不关于$ 0 $对称，并且在$ x&lt;0 $的情况下它的输出为$ 0 $。 由于这个函数在$ x&lt;0 $时梯度为$ 0 $，因此会出现一种$ Dead\\ ReLU $的问题，即当输入为负时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应，导致相应参数永远不会被更新。 这种情况可能是由于初始化不好，或者学习率过大导致改变幅度过大到一个$ Dead \\ ReLU $的地方再也出不去。为了解决这个问题通常加上一个偏置项在初始化中，避免开始就进入了$ Dead\\ ReLU $。 Leaky ReLU ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181814790.png) 对于前面的$ ReLU $的神经元梯度为$ 0 $的问题，我们可以使用$ Leaky\\ ReLU $的方式。 ELU ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181814689.png) 进一步使得输出接近关于$ 0 $对称，但同时指数的运算也带来了较大的运算量。 Maxout ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181814307.png) 通过在两个神经元后面再加上一层神经元，其中有$ k $个神经元，参数个数也成$ k $倍增加。 总结一下，多使用$ ReLU $，但是要注意它的学习率，不要在中间层使用$ sigmod $函数，可能是因为计算量太大还会有$ zig-zag $问题。 数据预处理 + 标准化处理 通过标准化，使其分布更加均匀，关于标准正态的解释可以使用中心极限定理解释，原始分布在$ n $足够大时近似为正态分布。 主成分分析 首先通过主成分分析将数据转化在一个二维空间中，再进行标准化，这也被称为$ PCA $白化，然后将其重新变回之前所在的维度，这被称为$ ZCA $白化。 权重初始化 ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181814503.png) 我们可能认为由于最终都有梯度下降进行迭代，那么无论怎么进行权重初始化都没有什么影响，但是实际上。如果我们给同一层的神经元一个相同的值，那么对于相同的输入，相同的权重，自然会导致相同的输出，相同的反向传播梯度，相同的更新，那么无论我们具有多少神经元本质上都是仅仅只有一个，因此权重初始化是一个很重要的问题。 当初始化的权重过小时，输出全部都分布在$ 0 $这个地方，那么就会导致梯度为$ 0 $。 当权重增大时，输出不全为$ 0 $，但是激活函数的导数全部为$ 0 $，那么梯度也为$ 0 $。 为了使得输入的方差和输出的方差相同，我们可以使用$ Xavier $的方式进行权重初始化。 使用将权重的方差设置为符合$ (0,\\frac{1}{d}) $的正态分布，也就可以保证输入的方差和输出的方差相同（但是上面的图我还是有些看不明白，既然保持不变那么为什么还会减小，直到最后几层才保持不变，是因为没有考虑激活函数的影响吗） 对于不是关于$ 0 $对称的$ ReLU $，可以使用$ Kaiming $初始化的方法。 批量归一化 ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181815457.png) 在进行深层的神经网络训练中，顶层的梯度很大，但是越往下层的梯度就越小，因此底层的收敛速度就很慢，因此在底层进行收敛的时候导致顶层原本学习好的数据重新学习，这无疑是低效的，因此我们希望可以每层的输出和梯度都保持在一个固定的分布。 具体的就是计算每个小批量的均值和方差，方差计算中需要加上一个$ \\epsilon $，避免计算时分母为$ 0 $的情况。同时可能标准正态分布并不满足要求，因此可以使用$ \\gamma,\\beta $来调整方差和均值。 当作用在输出上时，这个批量归一化的操作需要在激活函数之前，因为如果先进行了非线性的改变在进行批量归一化，例如$ ReLU $将很多输出都设置为$ 0 $，这时的意义不是很大。 当作用在输入时，这个批量归一化对于全连接来讲就是对于每一个特征计算对应的均值和方差，对于卷积层来讲就是将每个通道维的全部像素点作为样本，因此每个通道维就是特征，计算每个通道维的均值和方差。 对于批量归一化到底是在做什么不太好判断，一开始认为是减少内部变量的转移，后来又认为是在小批量中加入噪音。 批量归一化通过固定均值和方差，然后加上可调整的偏移和缩放，这样做可以使得训练加快，但是无法改变模型的精度本身。 优化算法 SGD ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181815598.png) 在开始介绍了梯度下降，随机梯度下降，小批量随机梯度下降这三种方法，但是对于这些方法来讲，除了使用全部数据的梯度下降，都存在震荡的情况。 更有甚者，可能会出现陷入局部最优点，鞍点的情况，对于这种情况，可以考虑加上一个动量。 SGD+Momentum ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181815539.png) 即不是仅仅使用梯度本身，而是使用一个$ v_t $，使用$ v_t={\\beta}v_{t-1}+g_t $，这样展开的话就是一个平滑的梯度，其中的$ \\beta $作为一个超参数，当批量较大时也可以设置一个较大的$ \\beta $，这样可以考虑更多的数据。 Nesterov Momentum ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181815139.png) 刚刚使用的$ Momentum $的方法很好用，但是还可以更好的改进，尽然要使用$ v_t $，那就使用加上$ v_t $之后的梯度，向前看一步，显然使用$ a $这个梯度是要好于$ b $的。 图中的点标注有点错误，应该是$ X_t+1 $。 AdaGrad ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181815657.png) 在$ AdaGrad $中使用了对梯度进行惩罚的方式，但是因为$ grad_squared $是一个累加和，随着这个数的越来越大在后面的更新可能会越来越慢。 $ RMSProp $ 为了解决上面的问题，引入了一个$ decay_{rate} $进行衰减，通过这个值选择保留多少之前的$ grad_squared $。 Adam ![](https://yzd-blog-oss.oss-cn-beijing.aliyuncs.com/img/202411181815855.png) 上面的$ Momentum $考虑了之前的累加和，$ AdaGrad $即考虑了对较大的梯度进行惩罚，现在将这两种方法加在一起就有了$ Adam $优化算法。 但是这个的第一动量和第二动量在开始的时候都起不到什么作用，因此可以进行优化。 开始的时候$ t $很小，因此$ 1\\ -\\ beta1\\ ^{**}\\ t $的值就很小，因此$ first_unbias $就很大，使得这个值在开始的时候不会不起作用，随着时间的累积，这个值和原本的$ first_moment $自然越来越接近。 上述提到的这些都是仅使用一阶导函数，还可以使用二阶导函数的牛顿法进行优化。","link":"/2024/09/21/%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"神经网络与反向传播","text":"首先我们需要讲之前的线性函数转化为非线性函数，也就是使用$ Relu(x)=max(0,x) $这个激活函数，多个这样的感知机加在一起，也就是我们所讲的多层感知机$ MLP $，也被叫做全连接神经网络。 正是有了激活函数，神经网络才可以拟合非线性，否则无论有多少层神经元都可以使用一个线性转换得到最后的结果，和单个线性层无区别。 我们通常使用的有以下几种激活函数，其中$ ReLU $最为常用，因为其结构简单，但是由于在输入为负时的结果梯度为$ 0 $，因此有了$ ELU,Leaky ReLU $作为优化。 在计算梯度时，需要使用计算图和反向传播，这在机器学习中也有具体解释。 对于不同的操作，梯度的变换也不尽相同，例如，加法时梯度没有改变，乘法时梯度发生交换。 注意：隐含层的作用就是将输入的非线性的数据转换为线性可分的数据，因此在输出层中就不再需要使用激活函数。","link":"/2024/07/14/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"}],"categories":[{"name":"blog","slug":"blog","link":"/categories/blog/"}],"pages":[{"title":"about","text":"🙋 Hello there 👋 I am a junior software engineering student with knowledge in Java backend development and currently learning about machine learning. 💼 Student at Zhengzhou Light Industry University. 🌱 Currently learning Machine Learning, Java, Operating Systems, Computer Networks, Data Structures, Algorithms, Principles of Computer Composition. 📚 Reading Computer Networks, Seventh Edition. 💻 Two or more years of computer science literacy studies. ⛵ Encouraging open-source collaborations. ✍🏻 Writing about Programming &amp; Tech on Personal Blog. 👑 GitHub statistical reports: Take a look at my repositories and let's get in touch!","link":"/about/index.html"}]}